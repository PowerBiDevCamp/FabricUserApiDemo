{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "a365ComputeOptions": null,
    "sessionKeepAliveTimeout": 0,
    "trident": {
      "lakehouse": {
        "default_lakehouse": "{LAKEHOUSE_ID}",
        "default_lakehouse_name": "{LAKEHOUSE_NAME}",
        "default_lakehouse_workspace_id": "{WORKSPACE_ID}",
        "known_lakehouses": [
          {
            "id": "{LAKEHOUSE_ID}"
          }
        ]
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
      },
      "source": [
        "# copy CSV files to lakehouse to load data into bronze layer \n",
        "import requests\n",
        "\n",
        "csv_base_url = \"https://github.com/PowerBiDevCamp/ProductSalesData/raw/main/\"\n",
        "\n",
        "csv_files = { \"Customers.csv\", \"Products.csv\", \"Invoices.csv\", \"InvoiceDetails.csv\" }\n",
        "\n",
        "folder_path = \"Files/bronze_landing_layer/\"\n",
        "\n",
        "for csv_file in csv_files:\n",
        "    csv_file_path = csv_base_url + csv_file\n",
        "    with requests.get(csv_file_path) as response:\n",
        "        csv_content = response.content.decode('utf-8-sig')\n",
        "        mssparkutils.fs.put(folder_path + csv_file, csv_content, True)\n",
        "        print(csv_file + \" copied to Lakehouse file in OneLake\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
      },
      "source": [
        "# create products table for silver layer\n",
        "from pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType\n",
        "\n",
        "# create schema for products table using StructType and StructField \n",
        "schema_products = StructType([\n",
        "    StructField(\"ProductId\", LongType() ),\n",
        "    StructField(\"Product\", StringType() ),\n",
        "    StructField(\"Category\", StringType() )\n",
        "])\n",
        "\n",
        "# Load CSV file into Spark DataFrame and validate data using schema\n",
        "df_products = (\n",
        "    spark.read.format(\"csv\")\n",
        "         .option(\"header\",\"true\")\n",
        "         .schema(schema_products)\n",
        "         .load(\"Files/bronze_landing_layer/Products.csv\")\n",
        ")\n",
        "\n",
        "# save DataFrame as lakehouse table in Delta format\n",
        "( df_products.write\n",
        "             .mode(\"overwrite\")\n",
        "             .option(\"overwriteSchema\", \"True\")\n",
        "             .format(\"delta\")\n",
        "             .save(\"Tables/silver_products\")\n",
        ")\n",
        "\n",
        "# display table schema and data\n",
        "df_products.printSchema()\n",
        "df_products.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
      },
      "source": [
        "# create customers table for silver layer\n",
        "from pyspark.sql.types import StructType, StructField, StringType, LongType, DateType\n",
        "\n",
        "# create schema for customers table using StructType and StructField \n",
        "schema_customers = StructType([\n",
        "    StructField(\"CustomerId\", LongType() ),\n",
        "    StructField(\"FirstName\", StringType() ),\n",
        "    StructField(\"LastName\", StringType() ),\n",
        "    StructField(\"Country\", StringType() ),\n",
        "    StructField(\"City\", StringType() ),\n",
        "    StructField(\"DOB\", DateType() ),\n",
        "])\n",
        "\n",
        "# Load CSV file into Spark DataFrame with schema and support to infer dates\n",
        "df_customers = (\n",
        "    spark.read.format(\"csv\")\n",
        "         .option(\"header\",\"true\")\n",
        "         .schema(schema_customers)\n",
        "         .option(\"dateFormat\", \"MM/dd/yyyy\")\n",
        "         .option(\"inferSchema\", \"true\")\n",
        "         .load(\"Files/bronze_landing_layer/Customers.csv\")\n",
        ")\n",
        "\n",
        "# save DataFrame as lakehouse table in Delta format\n",
        "( df_customers.write\n",
        "              .mode(\"overwrite\")\n",
        "              .option(\"overwriteSchema\", \"True\")\n",
        "              .format(\"delta\")\n",
        "              .save(\"Tables/silver_customers\")\n",
        ")\n",
        "\n",
        "# display table schema and data\n",
        "df_customers.printSchema()\n",
        "df_customers.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
      },
      "source": [
        "# create invoices table for silver layer\n",
        "from pyspark.sql.types import StructType, StructField, LongType, FloatType, DateType\n",
        "\n",
        "# create schema for invoices table using StructType and StructField \n",
        "schema_invoices = StructType([\n",
        "    StructField(\"InvoiceId\", LongType() ),\n",
        "    StructField(\"Date\", DateType() ),\n",
        "    StructField(\"TotalSalesAmount\", FloatType() ),\n",
        "    StructField(\"CustomerId\", LongType() )\n",
        "])\n",
        "\n",
        "# Load CSV file into Spark DataFrame with schema and support to infer dates\n",
        "df_invoices = (\n",
        "    spark.read.format(\"csv\")\n",
        "         .option(\"header\",\"true\")\n",
        "         .schema(schema_invoices)\n",
        "         .option(\"dateFormat\", \"MM/dd/yyyy\")\n",
        "         .option(\"inferSchema\", \"true\") \n",
        "         .load(\"Files/bronze_landing_layer/Invoices.csv\")\n",
        ")\n",
        "\n",
        "# save DataFrame as lakehouse table in Delta format\n",
        "( df_invoices.write\n",
        "             .mode(\"overwrite\")\n",
        "             .option(\"overwriteSchema\", \"True\")\n",
        "             .format(\"delta\")\n",
        "             .save(\"Tables/silver_invoices\")\n",
        ")\n",
        "\n",
        "# display table schema and data\n",
        "df_invoices.printSchema()\n",
        "df_invoices.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
      },
      "source": [
        "# create invoice_details table for silver layer\n",
        "from pyspark.sql.types import StructType, StructField, LongType, FloatType\n",
        "\n",
        "# create schema for invoice_details table using StructType and StructField \n",
        "schema_invoice_details = StructType([\n",
        "    StructField(\"Id\", LongType() ),\n",
        "    StructField(\"Quantity\", LongType() ),\n",
        "    StructField(\"SalesAmount\", FloatType() ),\n",
        "    StructField(\"InvoiceId\", LongType() ),\n",
        "    StructField(\"ProductId\", LongType() )\n",
        "])\n",
        "\n",
        "# Load CSV file into Spark DataFrame and validate data using schema\n",
        "df_invoice_details = (\n",
        "    spark.read.format(\"csv\")\n",
        "         .option(\"header\",\"true\")\n",
        "         .schema(schema_invoice_details)\n",
        "         .load(\"Files/bronze_landing_layer/InvoiceDetails.csv\")\n",
        ")\n",
        "\n",
        "# save DataFrame as lakehouse table in Delta format\n",
        "( df_invoice_details.write\n",
        "                    .mode(\"overwrite\")\n",
        "                    .option(\"overwriteSchema\", \"True\")\n",
        "                    .format(\"delta\")\n",
        "                    .save(\"Tables/silver_invoice_details\")\n",
        ")\n",
        "\n",
        "# display table schema and data\n",
        "df_invoice_details.printSchema()\n",
        "df_invoice_details.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
      },
      "source": [
        "# create products table for gold layer\n",
        "\n",
        "# load DataFrame from silver layer table\n",
        "df_gold_products = (\n",
        "    spark.read\n",
        "         .format(\"delta\")\n",
        "         .load(\"Tables/silver_products\")\n",
        ")\n",
        "\n",
        "# write DataFrame to new gold layer table \n",
        "( df_gold_products.write\n",
        "                  .mode(\"overwrite\")\n",
        "                  .option(\"overwriteSchema\", \"True\")\n",
        "                  .format(\"delta\")\n",
        "                  .save(\"Tables/products\")\n",
        ")\n",
        "\n",
        "# display table schema and data\n",
        "df_gold_products.printSchema()\n",
        "df_gold_products.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
      },
      "source": [
        "# create customers table for gold layer\n",
        "from pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n",
        "\n",
        "# load DataFrame from silver layer table and perform transforms\n",
        "df_gold_customers = (\n",
        "    spark.read\n",
        "         .format(\"delta\")\n",
        "         .load(\"Tables/silver_customers\")\n",
        "         .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n",
        "         .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n",
        "         .drop('FirstName', 'LastName')\n",
        ")\n",
        "\n",
        "# write DataFrame to new gold layer table \n",
        "( df_gold_customers.write\n",
        "                   .mode(\"overwrite\")\n",
        "                   .option(\"overwriteSchema\", \"True\")\n",
        "                   .format(\"delta\")\n",
        "                   .save(\"Tables/customers\")\n",
        ")\n",
        "\n",
        "# display table schema and data\n",
        "df_gold_customers.printSchema()\n",
        "df_gold_customers.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
      },
      "source": [
        "# create sales table for gold layer\n",
        "from pyspark.sql.functions import col, desc, concat, lit, floor, datediff\n",
        "from pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n",
        "\n",
        "# load DataFrames using invoices table and invoice_details table from silver layer\n",
        "df_silver_invoices = spark.read.format(\"delta\").load(\"Tables/silver_invoices\")\n",
        "df_silver_invoice_details = spark.read.format(\"delta\").load(\"Tables/silver_invoice_details\")\n",
        "\n",
        "# perform join to merge columns from both DataFrames and transform data \n",
        "df_gold_sales = (\n",
        "    df_silver_invoice_details\n",
        "        .join(df_silver_invoices, \n",
        "              df_silver_invoice_details['InvoiceId'] == df_silver_invoices['InvoiceId'])\n",
        "        .withColumnRenamed('SalesAmount', 'Sales')\n",
        "        .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n",
        "                               (month(col('Date'))*100) + \n",
        "                               (dayofmonth(col('Date')))   )\n",
        "        .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n",
        "        .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n",
        ")\n",
        "\n",
        "# write DataFrame to new gold layer table \n",
        "( df_gold_sales.write\n",
        "               .mode(\"overwrite\")\n",
        "               .option(\"overwriteSchema\", \"True\")\n",
        "               .format(\"delta\")\n",
        "               .save(\"Tables/sales\")\n",
        ")\n",
        "\n",
        "# display table schema and data\n",
        "df_gold_sales.printSchema()\n",
        "df_gold_sales.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
      },
      "source": [
        "# create calendar table for gold layer\n",
        "from datetime import date\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek, date_format\n",
        "\n",
        "# get first and last calendar date from sakes table \n",
        "first_sales_date = df_gold_sales.agg({\"Date\": \"min\"}).collect()[0][0]\n",
        "last_sales_date = df_gold_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n",
        "\n",
        "# calculate start date and end date for calendar table\n",
        "start_date = date(first_sales_date.year, 1, 1)\n",
        "end_date = date(last_sales_date.year, 12, 31)\n",
        "\n",
        "# create pandas DataFrame with Date series column\n",
        "df_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n",
        "\n",
        "# convert pandas DataFrame to Spark DataFrame and add calculated calendar columns\n",
        "df_calendar_spark = (\n",
        "     spark.createDataFrame(df_calendar_ps)\n",
        "       .withColumnRenamed(\"0\", \"timestamp\")\n",
        "       .withColumn(\"Date\", to_date(col('timestamp')))\n",
        "       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n",
        "                              (month(col('timestamp'))*100) + \n",
        "                              (dayofmonth(col('timestamp')))   )\n",
        "       .withColumn(\"Year\", year(col('timestamp'))  )\n",
        "       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-QQ\")  )\n",
        "       .withColumn(\"Month\", date_format(col('timestamp'),'yyyy-MM')  )\n",
        "       .withColumn(\"Day\", dayofmonth(col('timestamp'))  )\n",
        "       .withColumn(\"MonthInYear\", date_format(col('timestamp'),'MMMM')  )\n",
        "       .withColumn(\"MonthInYearSort\", month(col('timestamp'))  )\n",
        "       .withColumn(\"DayOfWeek\", date_format(col('timestamp'),'EEEE')  )\n",
        "       .withColumn(\"DayOfWeekSort\", dayofweek(col('timestamp')))\n",
        "       .drop('timestamp')\n",
        ")\n",
        "\n",
        "# write DataFrame to new gold layer table \n",
        "( df_calendar_spark.write\n",
        "                   .mode(\"overwrite\")\n",
        "                   .option(\"overwriteSchema\", \"True\")\n",
        "                   .format(\"delta\")\n",
        "                   .save(\"Tables/calendar\")\n",
        ")\n",
        "\n",
        "# display table schema and data\n",
        "df_calendar_spark.printSchema()\n",
        "df_calendar_spark.show()"
      ]
    }
  ]
}